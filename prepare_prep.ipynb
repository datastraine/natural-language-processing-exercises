{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inshorts = acquire.get_inshorts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_articles = acquire.get_blog_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Clean\n",
    "Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "* Lowercase everything\n",
    "* Normalize unicode characters\n",
    "* Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "somestring = \"The time is now! let'S do THis! LeeeRoory JenKins! that's right we showed you what is up!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(somestring):\n",
    "    basic = somestring.lower()\n",
    "    basic = unicodedata.normalize('NFKD', basic).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    basic = re.sub(r\"[^a-z0-9'\\s!]\", '', basic)\n",
    "    return basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the time is now! let's do this! leeeroory jenkins! that's right we showed you what is up!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_clean(somestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(somestring):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    tokened = tokenizer.tokenize(somestring, return_str=True)    \n",
    "    return tokened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The time is now ! let ' S do THis ! LeeeRoory JenKins ! that ' s right we showed you what is up !\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(somestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem\n",
    "Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(somestring):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in somestring.split()]\n",
    "    article_stemmed = ' '.join(stems)\n",
    "    return article_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the time is now! let' do this! leeeroori jenkins! that' right we show you what is up!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(somestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize\n",
    "Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(somestring):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in somestring.split()]\n",
    "    article_lemmatized = ' '.join(lemmas)\n",
    "    return article_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time is now! let'S do THis! LeeeRoory JenKins! that's right we showed you what is up!\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize(somestring))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove_stopwords\n",
    "Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anthony\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anthony\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time now! let's this! leeeroory jenkins! that's right showed up!\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(somestring, list_to_remove=[]):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list.remove('no')\n",
    "    stopword_list.remove('not')\n",
    "    somestring = somestring.lower()\n",
    "    words = somestring.split()\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "    return article_without_stopwords\n",
    "\n",
    "print(remove_stopwords(somestring))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "Use the acquire function to pull in the data and \n",
    "* Make a df for the news articles - news_df.\n",
    "* Make another df for the Codeup blog posts -codeup_df.\n",
    "\n",
    "* title to hold the title\n",
    "* original to hold the original article/post content\n",
    "* clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "* stemmed to hold the stemmed version of the cleaned data.\n",
    "* lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Moderna's early data shows its COVID-19 vaccin...</td>\n",
       "      <td>American biotechnology company Moderna on Mond...</td>\n",
       "      <td>american biotechnology company moderna monday ...</td>\n",
       "      <td>american biotechnolog compani moderna on monda...</td>\n",
       "      <td>American biotechnology company Moderna on Mond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15 countries sign world's biggest free-trade p...</td>\n",
       "      <td>Fifteen Asia-Pacific countries signed the Regi...</td>\n",
       "      <td>fifteen asiapacific countries signed regional ...</td>\n",
       "      <td>fifteen asia-pacif countri sign the region com...</td>\n",
       "      <td>Fifteen Asia-Pacific country signed the Region...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does Moderna's COVID-19 vaccine candidate ...</td>\n",
       "      <td>Moderna's initial results of late-stage trial ...</td>\n",
       "      <td>moderna ' initial results latestage trial show...</td>\n",
       "      <td>moderna' initi result of late-stag trial show ...</td>\n",
       "      <td>Moderna's initial result of late-stage trial s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reliance Retail buys 96% stake in Urban Ladder...</td>\n",
       "      <td>Reliance Industries' retail arm Reliance Retai...</td>\n",
       "      <td>reliance industries ' retail arm reliance reta...</td>\n",
       "      <td>relianc industries' retail arm relianc retail ...</td>\n",
       "      <td>Reliance Industries' retail arm Reliance Retai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reduce foreign funding to 26% by Oct 15, 2021:...</td>\n",
       "      <td>The I&amp;B Ministry on Monday asked digital media...</td>\n",
       "      <td>ib ministry monday asked digital media compani...</td>\n",
       "      <td>the i&amp;b ministri on monday ask digit media com...</td>\n",
       "      <td>The I&amp;B Ministry on Monday asked digital mediu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>My role in Laxmii was a cameo but it had a lot...</td>\n",
       "      <td>Speaking about his cameo in Akshay Kumar-starr...</td>\n",
       "      <td>speaking cameo akshay kumarstarrer ' laxmii ' ...</td>\n",
       "      <td>speak about hi cameo in akshay kumar-starr 'la...</td>\n",
       "      <td>Speaking about his cameo in Akshay Kumar-starr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Lost a family member: Satyajit Ray's son on So...</td>\n",
       "      <td>Following the demise of Bengali actor Soumitra...</td>\n",
       "      <td>following demise bengali actor soumitra chatte...</td>\n",
       "      <td>follow the demis of bengali actor soumitra cha...</td>\n",
       "      <td>Following the demise of Bengali actor Soumitra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Matrix 4 breaks Germany's COVID-19 laws by hos...</td>\n",
       "      <td>The team of Keanu Reeves' upcoming film 'The M...</td>\n",
       "      <td>team keanu reeves ' upcoming film ' matrix 4 '...</td>\n",
       "      <td>the team of keanu reeves' upcom film 'the matr...</td>\n",
       "      <td>The team of Keanu Reeves' upcoming film 'The M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>He was one of my oldest friends: Sharmila on S...</td>\n",
       "      <td>Following veteran Bengali actor Soumitra Chatt...</td>\n",
       "      <td>following veteran bengali actor soumitra chatt...</td>\n",
       "      <td>follow veteran bengali actor soumitra chatterj...</td>\n",
       "      <td>Following veteran Bengali actor Soumitra Chatt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Aditya made me feel comfortable on the sets of...</td>\n",
       "      <td>Speaking about her experience of working with ...</td>\n",
       "      <td>speaking experience working aditya roy kapur l...</td>\n",
       "      <td>speak about her experi of work with aditya roy...</td>\n",
       "      <td>Speaking about her experience of working with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Moderna's early data shows its COVID-19 vaccin...   \n",
       "1   15 countries sign world's biggest free-trade p...   \n",
       "2   How does Moderna's COVID-19 vaccine candidate ...   \n",
       "3   Reliance Retail buys 96% stake in Urban Ladder...   \n",
       "4   Reduce foreign funding to 26% by Oct 15, 2021:...   \n",
       "..                                                ...   \n",
       "94  My role in Laxmii was a cameo but it had a lot...   \n",
       "95  Lost a family member: Satyajit Ray's son on So...   \n",
       "96  Matrix 4 breaks Germany's COVID-19 laws by hos...   \n",
       "97  He was one of my oldest friends: Sharmila on S...   \n",
       "98  Aditya made me feel comfortable on the sets of...   \n",
       "\n",
       "                                             original  \\\n",
       "0   American biotechnology company Moderna on Mond...   \n",
       "1   Fifteen Asia-Pacific countries signed the Regi...   \n",
       "2   Moderna's initial results of late-stage trial ...   \n",
       "3   Reliance Industries' retail arm Reliance Retai...   \n",
       "4   The I&B Ministry on Monday asked digital media...   \n",
       "..                                                ...   \n",
       "94  Speaking about his cameo in Akshay Kumar-starr...   \n",
       "95  Following the demise of Bengali actor Soumitra...   \n",
       "96  The team of Keanu Reeves' upcoming film 'The M...   \n",
       "97  Following veteran Bengali actor Soumitra Chatt...   \n",
       "98  Speaking about her experience of working with ...   \n",
       "\n",
       "                                                clean  \\\n",
       "0   american biotechnology company moderna monday ...   \n",
       "1   fifteen asiapacific countries signed regional ...   \n",
       "2   moderna ' initial results latestage trial show...   \n",
       "3   reliance industries ' retail arm reliance reta...   \n",
       "4   ib ministry monday asked digital media compani...   \n",
       "..                                                ...   \n",
       "94  speaking cameo akshay kumarstarrer ' laxmii ' ...   \n",
       "95  following demise bengali actor soumitra chatte...   \n",
       "96  team keanu reeves ' upcoming film ' matrix 4 '...   \n",
       "97  following veteran bengali actor soumitra chatt...   \n",
       "98  speaking experience working aditya roy kapur l...   \n",
       "\n",
       "                                              stemmed  \\\n",
       "0   american biotechnolog compani moderna on monda...   \n",
       "1   fifteen asia-pacif countri sign the region com...   \n",
       "2   moderna' initi result of late-stag trial show ...   \n",
       "3   relianc industries' retail arm relianc retail ...   \n",
       "4   the i&b ministri on monday ask digit media com...   \n",
       "..                                                ...   \n",
       "94  speak about hi cameo in akshay kumar-starr 'la...   \n",
       "95  follow the demis of bengali actor soumitra cha...   \n",
       "96  the team of keanu reeves' upcom film 'the matr...   \n",
       "97  follow veteran bengali actor soumitra chatterj...   \n",
       "98  speak about her experi of work with aditya roy...   \n",
       "\n",
       "                                           lemmatized  \n",
       "0   American biotechnology company Moderna on Mond...  \n",
       "1   Fifteen Asia-Pacific country signed the Region...  \n",
       "2   Moderna's initial result of late-stage trial s...  \n",
       "3   Reliance Industries' retail arm Reliance Retai...  \n",
       "4   The I&B Ministry on Monday asked digital mediu...  \n",
       "..                                                ...  \n",
       "94  Speaking about his cameo in Akshay Kumar-starr...  \n",
       "95  Following the demise of Bengali actor Soumitra...  \n",
       "96  The team of Keanu Reeves' upcoming film 'The M...  \n",
       "97  Following veteran Bengali actor Soumitra Chatt...  \n",
       "98  Speaking about her experience of working with ...  \n",
       "\n",
       "[99 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame(acquire.get_inshorts())\n",
    "news_df = news_df[['title', 'content']]\n",
    "news_df.columns=['title', 'original']\n",
    "news_df['clean'] = news_df['original'].apply(basic_clean).apply(tokenize).apply(remove_stopwords)\n",
    "news_df['stemmed'] = news_df['original'].apply(stem)\n",
    "news_df['lemmatized'] = news_df['original'].apply(lemmatize)\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "      <td>rumors true ! time arrived codeup officially o...</td>\n",
       "      <td>the rumor are true! the time ha arrived. codeu...</td>\n",
       "      <td>The rumor are true! The time ha arrived. Codeu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "      <td>dimitri antoniou maggie giust data science big...</td>\n",
       "      <td>By dimitri antoni and maggi giust data science...</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust Data Scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "      <td>dimitri antoniou week ago codeup launched imme...</td>\n",
       "      <td>By dimitri antoni A week ago, codeup launch ou...</td>\n",
       "      <td>By Dimitri Antoniou A week ago, Codeup launche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "      <td>sa tech job fair third biannual san antonio te...</td>\n",
       "      <td>SA tech job fair the third bi-annu san antonio...</td>\n",
       "      <td>SA Tech Job Fair The third bi-annual San Anton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>competitor bootcamps closing model danger prog...</td>\n",
       "      <td>competitor bootcamp are closing. Is the model ...</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!   \n",
       "1                                 Data Science Myths   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                            original  \\\n",
       "0  The rumors are true! The time has arrived. Cod...   \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...   \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...   \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  rumors true ! time arrived codeup officially o...   \n",
       "1  dimitri antoniou maggie giust data science big...   \n",
       "2  dimitri antoniou week ago codeup launched imme...   \n",
       "3  sa tech job fair third biannual san antonio te...   \n",
       "4  competitor bootcamps closing model danger prog...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  the rumor are true! the time ha arrived. codeu...   \n",
       "1  By dimitri antoni and maggi giust data science...   \n",
       "2  By dimitri antoni A week ago, codeup launch ou...   \n",
       "3  SA tech job fair the third bi-annu san antonio...   \n",
       "4  competitor bootcamp are closing. Is the model ...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  The rumor are true! The time ha arrived. Codeu...  \n",
       "1  By Dimitri Antoniou and Maggie Giust Data Scie...  \n",
       "2  By Dimitri Antoniou A week ago, Codeup launche...  \n",
       "3  SA Tech Job Fair The third bi-annual San Anton...  \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df = pd.DataFrame(acquire.get_blog_articles())\n",
    "codeup_df.columns=['title', 'original']\n",
    "codeup_df['clean'] = codeup_df['original'].apply(basic_clean).apply(tokenize).apply(remove_stopwords)\n",
    "codeup_df['stemmed'] = codeup_df['original'].apply(stem)\n",
    "codeup_df['lemmatized'] = codeup_df['original'].apply(lemmatize)\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask yourself:\n",
    "\n",
    "* If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "  - A: Lemmatized as stemming is brutal and will make the results hard to read and the amount of text is small\n",
    "* If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "  - A : Lemmatized as stemming is brutal and will make the results hard to read and the amount of text is small; however, this will take slower machines longer to run\n",
    "* If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n",
    "  - A: Probably neither. The size makes lemmatizing too resource intensive, but stemming would make the results too hard to process. If it is necessary to do this step in the process then stemming is prefered unless the amount avaiable CPU is high enough to do the task OR this is a once in a while run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
